{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda00e85-f02d-402d-8f38-a36341733b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ddf5157-04d0-4e88-8c62-7fb0995ec255",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbb25f2-7625-47b9-81ba-af284a84896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed6d832-15c5-4b76-bdf1-a641c06a2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ccd583-3527-41ea-a5d8-12a75701a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "085bb7fb-ac5f-493c-bc47-d88c6584c7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'BAAI/bge-base-en',\n",
       "  'sources': {'hf': 'Qdrant/fast-bge-base-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.42,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-base-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-base-en-v1.5-onnx-q',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.21,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-large-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-large-en-v1.5-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-small-en-v1.5-onnx-q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.067,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-zh-v1.5',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-zh-v1.5',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'mixedbread-ai/mxbai-embed-large-v1',\n",
       "  'sources': {'hf': 'mixedbread-ai/mxbai-embed-large-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-xs',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-xs',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-s',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-s',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-m',\n",
       "  'sources': {'hf': 'Snowflake/snowflake-arctic-embed-m',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.43,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-m-long',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-m-long',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 2048 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.54,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-l',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-l',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 1.02,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-clip-v1',\n",
       "  'sources': {'hf': 'jinaai/jina-clip-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/text_model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text&image), English, Prefixes for queries/documents: not necessary, 2024 year',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.55,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'Qdrant/clip-ViT-B-32-text',\n",
       "  'sources': {'hf': 'Qdrant/clip-ViT-B-32-text',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.25,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'sources': {'hf': 'qdrant/all-MiniLM-L6-v2-onnx',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-en',\n",
       "  'sources': {'hf': 'xenova/jina-embeddings-v2-base-en',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-small-en',\n",
       "  'sources': {'hf': 'xenova/jina-embeddings-v2-small-en',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.12,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-de',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-de',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model_fp16.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (German, English), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.32,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-code',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-code',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (English, 30 programming languages), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-zh',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-zh',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), supports mixed Chinese-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-es',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-es',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), supports mixed Spanish-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'thenlper/gte-base',\n",
       "  'sources': {'hf': 'thenlper/gte-base',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'General text embeddings, Unimodal (text), supports English only input text, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.44,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'thenlper/gte-large',\n",
       "  'sources': {'hf': 'qdrant/gte-large-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1.5-Q',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model_quantized.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'sources': {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2019 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.22,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "  'sources': {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 384 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 1.0,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'intfloat/multilingual-e5-large',\n",
       "  'sources': {'hf': 'qdrant/multilingual-e5-large-onnx',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~100 languages), 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 2.24,\n",
       "  'additional_files': ['model.onnx_data'],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v3',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v3',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Multi-task unimodal (text) embedding model, multi-lingual (~100), 1024 tokens truncation, and 8192 sequence length. Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'cc-by-nc-4.0',\n",
       "  'size_in_GB': 2.29,\n",
       "  'additional_files': ['onnx/model.onnx_data'],\n",
       "  'dim': 1024,\n",
       "  'tasks': {'retrieval.query': 0,\n",
       "   'retrieval.passage': 1,\n",
       "   'separation': 2,\n",
       "   'classification': 3,\n",
       "   'text-matching': 4}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d07fdd2-9d62-48e2-af92-67478adef9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'BAAI/bge-small-zh-v1.5', 'sources': {'hf': 'Qdrant/bge-small-zh-v1.5', 'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz', '_deprecated_tar_struct': True}, 'model_file': 'model_optimized.onnx', 'description': 'Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.', 'license': 'mit', 'size_in_GB': 0.09, 'additional_files': [], 'dim': 512, 'tasks': {}}\n",
      "{'model': 'Qdrant/clip-ViT-B-32-text', 'sources': {'hf': 'Qdrant/clip-ViT-B-32-text', 'url': None, '_deprecated_tar_struct': False}, 'model_file': 'model.onnx', 'description': 'Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year', 'license': 'mit', 'size_in_GB': 0.25, 'additional_files': [], 'dim': 512, 'tasks': {}}\n",
      "{'model': 'jinaai/jina-embeddings-v2-small-en', 'sources': {'hf': 'xenova/jina-embeddings-v2-small-en', 'url': None, '_deprecated_tar_struct': False}, 'model_file': 'onnx/model.onnx', 'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.', 'license': 'apache-2.0', 'size_in_GB': 0.12, 'additional_files': [], 'dim': 512, 'tasks': {}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "for model in TextEmbedding.list_supported_models():\n",
    "    if model[\"dim\"] == EMBEDDING_DIMENSIONALITY:\n",
    "        print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9028529f-4d92-4261-9040-7ba426489669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b70d0675-6d75-4ab6-b518-9267194baaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a collection for indexing the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32e35162-8bb6-45e3-b962-03b64bd698f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"zoomcamo-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810992c0-892c-4d0a-ae62-031a62af6ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(collection_name=collection_name,\n",
    "                         vectors_config=models.VectorParams(\n",
    "                             size=EMBEDDING_DIMENSIONALITY,\n",
    "                             distance=models.Distance.COSINE\n",
    "                         )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f05e4ec8-cc9d-46ae-94f0-fcae032a8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the data points, and call them points\n",
    "points = []\n",
    "id = 0\n",
    "for course in documents_raw:\n",
    "    for doc in course['documents']:\n",
    "        point = models.PointStruct(\n",
    "            id=id,\n",
    "            vector=models.Document(text=str(doc['text']), model=model_handle),\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course['course']\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55edc4e6-d540-4608-ad61-504fad57fab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21de6dee594c40f19ce231ebbd215ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f0a3f5dec64d1b889e7c89e83f3f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994feb78c6ca4ea8be432efcfa31f46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fc5bb69e0b4705872e6b17596e37ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8063024c11e24ae392af3c209771be0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134e387c32454ae5b722036279a1cdbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(collection_name=collection_name,\n",
    "              points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef164150-7a4a-4f10-a271-6dc0a6f00b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, limit:int=1):\n",
    "    results = client.query_points(collection_name=collection_name,\n",
    "                                  query=models.Document(\n",
    "                                      text=query,\n",
    "                                      model=model_handle\n",
    "                                  ),\n",
    "                                  limit=limit,\n",
    "                                  with_payload=True\n",
    "                                 )\n",
    "    return results\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39aedba5-eadf-49d0-bd94-78e4cd297c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"text\": \"I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\\nThe error message is:\\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\\nI solved it by performing the homework directly as a python script.\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\\nYou can try using the Pyspark library\\nAnswered by kamaldeen (kamaldeen32@gmail.com)\",\n",
      " \"section\": \"Module 1: Introduction\",\n",
      " \"question\": \"Reading large parquet files\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Lets get a random question\n",
    "import random\n",
    "random.seed(42)\n",
    "course = random.choice(documents_raw)\n",
    "course_piece = random.choice(course['documents'])\n",
    "print(json.dumps(course_piece, indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04c195bf-d986-4e34-a251-76cd6929f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search(course_piece['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34963a0f-828e-4855-abc0-c7e2242145d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.\n"
     ]
    }
   ],
   "source": [
    "print(result.points[0].payload['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "952c84bd-797a-4e23-a72e-d09ae3c32f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_filter(query, filter_name: str='course', filter_value: str='mlops-zoomcamp', limit:int=1):\n",
    "    results = client.query_points(collection_name=collection_name,\n",
    "                                  query=models.Document(\n",
    "                                      text=query,\n",
    "                                      model=model_handle\n",
    "                                  ),\n",
    "                                  query_filter=models.Filter(\n",
    "                                      must=[\n",
    "                                          models.FieldCondition(\n",
    "                                              key=filter_name,\n",
    "                                              match=models.MatchValue(value=filter_value)\n",
    "                                          )\n",
    "                                      ]\n",
    "                                  ),\n",
    "                                  limit=limit,\n",
    "                                  with_payload=True\n",
    "                                 )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c6fad5b-d323-4b01-aa44-b0ff74a56996",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=search_by_filter(query=course_piece['question'],\n",
    "                        filter_value=course['course'], \n",
    "                        limit=2\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c841b119-293f-4391-b851-a83f929e2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\\nThe error message is:\\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\\nI solved it by performing the homework directly as a python script.\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\\nYou can try using the Pyspark library\\nAnswered by kamaldeen (kamaldeen32@gmail.com)', 'section': 'Module 1: Introduction', 'course': 'mlops-zoomcamp'}\n",
      "{'text': \"Problem: While following the steps in the videos you may have problems trying to download with wget the files. Usually it is a 403 error type (Forbidden access).\\nSolution: The links point to files on cloudfront.net, something like this:\\nhttps://d37ci6vzurychx.cloudfront.net/tOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet OSError: Could not open parquet input source '<Buffer>': Invalid: Parquet rip+data/green_tripdata_2021-01.parquet\\nI’m not download the dataset directly, i use dataset URL and run this in the file.\\nUpdate(27-May-2023): Vikram\\nI am able to download the data from the below link. This is from the official  NYC trip record page (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Copy link from page directly as the below url might get changed if the NYC decides to move away from this. Go to the page , right click and use copy link.\\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\\n(Asif)\\nCopy the link address and replace the cloudfront.net part with s3.amazonaws.com/nyc-tlc/, so it looks like this:\\nhttps://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet\\nMario Tormo (mario@tormo-romero.eu)\\nOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet\", 'section': 'Module 1: Introduction', 'course': 'mlops-zoomcamp'}\n"
     ]
    }
   ],
   "source": [
    "for result in results.points:\n",
    "    print(result.payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "421bce44-17d3-4a89-84cf-6973bb19d3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\n",
      "The error message is:\n",
      "IndexError: index 311297 is out of bounds for axis 0 with size 131743\n",
      "I solved it by performing the homework directly as a python script.\n",
      "Added by Ibraheem Taha (ibraheemtaha91@gmail.com)\n",
      "You can try using the Pyspark library\n",
      "Answered by kamaldeen (kamaldeen32@gmail.com)\n"
     ]
    }
   ],
   "source": [
    "print(result.points[0].payload['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee1be433-f6f9-4016-a21c-93a657d9221d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In order to obtain the certificate, completion of the final capstone project is mandatory. The completion of weekly homework assignments is optional, but they can contribute to your overall progress and ranking on the top 100 leaderboard.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_by_filter(query=\"Can I submit homework late?\").points[0].payload['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dd21a81-acde-4eed-93eb-4842b1598439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd21b64f-a83c-4ce5-b6cf-03cb8ff88985",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME_2 = \"zoomcamp-sparse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "682cba66-6f3d-42b4-b20b-bceb2811bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoomcamp-sparse, collection already exists\n"
     ]
    }
   ],
   "source": [
    "if not client.collection_exists(COLLECTION_NAME_2):\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME_2,\n",
    "        sparse_vectors_config={\n",
    "            \"bm25\": models.SparseVectorParams(\n",
    "                modifier=models.Modifier.IDF\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    print(f\"{COLLECTION_NAME_2}, collection already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f6a7996-24fe-4b57-a7f1-24c504424a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending the documents data as a sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca958846-62f2-4bff-9a75-eba4f8655891",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"course\": \"ml\", \"data\": [1,2,3,4]}, {\"course\": \"de\", \"data\": [1,2]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b6129b9-0f0a-49c6-860b-907c211f683a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', 1), ('ml', 2), ('ml', 3), ('ml', 4), ('de', 1), ('de', 2)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k[\"course\"],i) for k in data for i in k[\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f18b17d-bad8-4544-82d4-d6b639d7a232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a364cc915c84878ae1acb890f52a796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af47563675aa4744bcd2aebf474c4ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "danish.txt:   0%|          | 0.00/424 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1dd26c21da4a54aa9a9abfa2d59bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "french.txt:   0%|          | 0.00/813 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6229e7f1a6d747d1804382414b39727c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dutch.txt:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e989202ccf144b6b08d94e19bc34198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dd11162cff43e79728c049d58eafd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "finnish.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad44128c3cc342fa9f807b450b4789d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "english.txt:   0%|          | 0.00/936 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c37f64004b46079584d8d4b6d80556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "german.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddca94a53a2746dabb431ab59832379d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "arabic.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1376051ad249dc8d32eb57cbe58424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "greek.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48794abb7ccf4687b2ee1a642af94662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "portuguese.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f24ace496da41ef9817df0f2aa73ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "norwegian.txt:   0%|          | 0.00/851 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecf3c6372274f7cae5147ca0e8e9207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "italian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f363cc51d2d418ebd0661b02947c207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hungarian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874d8e6c68c44ae4ae5e4685a0a1dc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "romanian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf36c81b9d774443b6ec9e408bc9f372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "russian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c92ea7470ba4ceea475aaca2c41ad98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spanish.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1480537dc3ef40c0a2d6386932baa8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "swedish.txt:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57960898ef7f4fd68ad12ee306c748ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "turkish.txt:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=COLLECTION_NAME_2,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"],\n",
    "                    model=\"Qdrant/bm25\"\n",
    "                )\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course[\"course\"]\n",
    "            }\n",
    "        )\n",
    "        for course in documents_raw for doc in course[\"documents\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0bfc6376-4bfd-41ac-9a9f-b0a6a0e7b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sparse(query: str, limit: int=1) -> list[models.ScoredPoint]:\n",
    "    results = client.query_points(\n",
    "        collection_name=COLLECTION_NAME_2,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\"\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22243c9a-ac00-4c64-a02a-8dd5aafe1823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_sparse(\"when does the course start?\")[0].payload[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0461ac5-4243-4556-a45e-f34bbeb9e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_SPARSE_AND_DENSE = \"zoomcamp-sparse-and-dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3acb3779-0f16-4d25-bff3-e97c7cc3c0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection zoomcamp-sparse-and-dense already exists\n"
     ]
    }
   ],
   "source": [
    "if not client.collection_exists(COLLECTION_SPARSE_AND_DENSE):\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_SPARSE_AND_DENSE,\n",
    "        vectors_config={\n",
    "            \"jina-small\": models.VectorParams(\n",
    "                size=512,\n",
    "                distance=models.Distance.COSINE\n",
    "            )\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            \"bm25\": models.SparseVectorParams(\n",
    "                modifier=models.Modifier.IDF\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    print(f\"Collection {COLLECTION_SPARSE_AND_DENSE} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4aed572-105b-40a9-9e7a-005c846ba450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=COLLECTION_SPARSE_AND_DENSE,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"jina-small\": models.Document(\n",
    "                    text=doc[\"text\"],\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\"\n",
    "                ),\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"],\n",
    "                    model=\"Qdrant/bm25\"\n",
    "                )\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course[\"course\"]\n",
    "            }\n",
    "        ) for course in documents_raw for doc in course[\"documents\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aeba26a3-ab43-4b2b-9558-2cb04199ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_stage_search(query, limit: int=2):\n",
    "    results = client.query_points(\n",
    "        collection_name=COLLECTION_SPARSE_AND_DENSE,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(text=query,\n",
    "                                      model=\"jinaai/jina-embeddings-v2-small-en\"\n",
    "                                     ),\n",
    "                limit=(limit*5),\n",
    "                using=\"jina-small\"\n",
    "            )\n",
    "        ],\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\"\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd5d29d8-e49d-4962-800c-46375b033bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\n",
      "Numpy\n",
      "Pandas\n",
      "pandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\n",
      "import numpy as np\n",
      "np.std(df.weight, ddof=1)\n",
      "The result will be similar if we change the dof = 1 in numpy\n",
      "(Harish Balasundaram)\n",
      "\n",
      "If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\n",
      "(Quinn Avila)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in multi_stage_search(\"pandas\"):\n",
    "    print(q.payload[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65855a95-9fce-4b68-ab6b-89b7a938cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_hybrid_search(query, limit: int=2):\n",
    "    results = client.query_points(\n",
    "        collection_name=COLLECTION_SPARSE_AND_DENSE,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(text=query,\n",
    "                                      model=\"jinaai/jina-embeddings-v2-small-en\"\n",
    "                                     ),\n",
    "                limit=(limit*5),\n",
    "                using=\"jina-small\"\n",
    "            )\n",
    "            ,\n",
    "            models.Prefetch(\n",
    "                query=models.Document(text=query,\n",
    "                                      model=\"Qdrant/bm25\"\n",
    "                                     ),\n",
    "                using=\"bm25\",\n",
    "                limit=(limit*5)\n",
    "            )\n",
    "        ],\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True\n",
    "    )\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14a352da-f6af-469d-8d28-81f98d997af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\n",
      "df_train_combined = pd.concat([df_train, df_val])\n",
      "y_train = np.concatenate((y_train, y_val), axis=0)\n",
      "(George Chizhmak)\n",
      "\n",
      "Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\n",
      "Numpy\n",
      "Pandas\n",
      "pandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\n",
      "import numpy as np\n",
      "np.std(df.weight, ddof=1)\n",
      "The result will be similar if we change the dof = 1 in numpy\n",
      "(Harish Balasundaram)\n",
      "\n",
      "You can use round() function or f-strings\n",
      "round(number, 4)  - this will round number up to 4 decimal places\n",
      "print(f'Average mark for the Homework is {avg:.3f}') - using F string\n",
      "Also there is pandas.Series. round idf you need to round values in the whole Series\n",
      "Please check the documentation\n",
      "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\n",
      "Added by Olga Rudakova\n",
      "\n",
      "Pandas and numpy libraries are not being imported\n",
      "NameError: name 'np' is not defined\n",
      "NameError: name 'pd' is not defined\n",
      "If you're using numpy or pandas, make sure you use the first few lines before anything else.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "Added by Manuel Alejandro Aponte\n",
      "\n",
      "This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\n",
      "because as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\n",
      "So when you run the command spark.createDataFrame(df1_pandas).show(),\n",
      "You get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\n",
      "Another option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\n",
      "pd.DataFrame.iteritems = pd.DataFrame.items\n",
      "Note that this problem is solved with Spark versions from 3.4.1\n",
      "\n",
      "If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\n",
      "(Quinn Avila)\n",
      "\n",
      "Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\n",
      "pandas.read_csv — pandas 2.1.4 documentation (pydata.org)\n",
      "Example from week 1\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\n",
      "'yellow_tripdata_2021-01.csv',\n",
      "nrows=100,\n",
      "parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
      "df.info()\n",
      "which will output\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 18 columns):\n",
      "#   Column                 Non-Null Count  Dtype\n",
      "---  ------                 --------------  -----\n",
      "0   VendorID               100 non-null    int64\n",
      "1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n",
      "2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n",
      "3   passenger_count        100 non-null    int64\n",
      "4   trip_distance          100 non-null    float64\n",
      "5   RatecodeID             100 non-null    int64\n",
      "6   store_and_fwd_flag     100 non-null    object\n",
      "7   PULocationID           100 non-null    int64\n",
      "8   DOLocationID           100 non-null    int64\n",
      "9   payment_type           100 non-null    int64\n",
      "10  fare_amount            100 non-null    float64\n",
      "11  extra                  100 non-null    float64\n",
      "12  mta_tax                100 non-null    float64\n",
      "13  tip_amount             100 non-null    float64\n",
      "14  tolls_amount           100 non-null    float64\n",
      "15  improvement_surcharge  100 non-null    float64\n",
      "16  total_amount           100 non-null    float64\n",
      "17  congestion_surcharge   100 non-null    float64\n",
      "dtypes: datetime64[ns](2), float64(9), int64(6), object(1)\n",
      "memory usage: 14.2+ KB\n",
      "\n",
      "{t_end - t_start} seconds\")\n",
      "import pandas as pd\n",
      "df = pd.read_csv('path/to/file.csv.gz', /app/ingest_data.py:1: DeprecationWarning:)\n",
      "If you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.\n",
      "\n",
      "Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\n",
      "✅Solution: Defined the schema while running web_to_gcp.py pipeline.\n",
      "Sebastian adapted the script:\n",
      "https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\n",
      "Need a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\n",
      "file_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\n",
      "open(file_name_gz, 'wb').write(r.content)\n",
      "os.system(f\"gzip -d {file_name_gz}\")\n",
      "os.system(f\"rm {file_name_init}.*\")\n",
      "Same ERROR - When running dbt run for fact_trips.sql, the task failed with error:\n",
      "“Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”\n",
      "开启屏幕阅读器支持\n",
      "要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\n",
      "查找和替换\n",
      "Reason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\n",
      "There are some possible fixes:\n",
      "Drop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\n",
      "SELECT * EXCEPT (ehail_fee) FROM…\n",
      "Modify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\n",
      "Modify Airflow dag to make the conversion and avoid the error.\n",
      "pv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))\n",
      "Same type of ERROR - parquet files with different data types - Fix it with pandas\n",
      "Here is another possibility that could be interesting:\n",
      "You can specify the dtypes when importing the file from csv to a dataframe with pandas\n",
      "pd.from_csv(..., dtype=type_dict)\n",
      "One obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\n",
      "Sources:\n",
      "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
      "Nullable integer data type — pandas 1.5.3 documentation\n",
      "\n",
      "How do I read the dataset with Pandas in Windows?\n",
      "I used the code below but not working\n",
      "df = pd.read_csv('C:\\Users\\username\\Downloads\\data.csv')\n",
      "Unlike Linux/Mac OS, Windows uses the backslash (\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the '\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\n\" to add a new line or \"\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\n",
      "Here’s how we should be loading the file instead:\n",
      "df = pd.read_csv(r'C:\\Users\\username\\Downloads\\data.csv')\n",
      "(Muhammad Awon)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in rrf_hybrid_search(\"pandas\"):\n",
    "    print(q.payload[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13de84-f72e-4cdf-b9ed-57464c9bd9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
